{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prostate MRI Dataset\n",
    "\n",
    "* Cross-validation by patient\n",
    "* Load feature map with ADC, CDI, Zones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%run -i LoadZoneData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "save_filename = 'raw_data'\n",
    "\n",
    "dir_path = os.path.join('.','data', 'data','data')\n",
    "#data = import_data(dir_path)\n",
    "#save_obj(data, save_filename)\n",
    "\n",
    "# Drop these patients (incomplete data)\n",
    "\n",
    "#drop_patients = ['P00000015', 'P00000249', 'P00000429']\n",
    "#drop_ids = []\n",
    "#for i,e in reversed(list(enumerate(data))):\n",
    "#    if data[i]['id'] in drop_patients:\n",
    "#        print(i)\n",
    "#        del data[i]\n",
    "\n",
    "# Resave the data object without those patients        \n",
    "#save_obj(data, save_filename)\n",
    "data = load_obj(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dictionary of models our script can use, where the key\n",
    "# to the dictionary is the name of the model (supplied via command\n",
    "# line argument) and the value is the model itself\n",
    "models = {\n",
    "\t\"knn\": KNeighborsClassifier(n_neighbors=1),\n",
    "\t\"naive_bayes\": GaussianNB(),\n",
    "\t\"logit\": LogisticRegression(solver=\"lbfgs\"),\n",
    "\t\"svm\": SVC(kernel=\"linear\"),\n",
    "\t\"decision_tree\": DecisionTreeClassifier(),\n",
    "\t\"random_forest\": RandomForestClassifier(n_estimators=100),\n",
    "\t\"mlp\": MLPClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dict = {}\n",
    "with open('5folds.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        pid, fold_no = line.split()\n",
    "        fold_dict[str(pid)] = int(fold_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = {}\n",
    "labels = []\n",
    "\n",
    "num_features = 2\n",
    "#patient = data[0]\n",
    "for modality in ['ADC']: # TODO: add T2-weighted images (if we get the labels)\n",
    "    \n",
    "    if modality == 'ADC':\n",
    "        label_map = 'maxGleason_map'\n",
    "        zone_map = 'zone_map'\n",
    "        \n",
    "    examples[modality] = []\n",
    "    \n",
    "    for _ in range(5):\n",
    "        examples[modality].append([])\n",
    "        labels.append([])\n",
    "\n",
    "    for i in range(5): \n",
    "        for _ in range(num_features+2): # num features (1 mod, 1 zones)\n",
    "            examples[modality][i].append([])\n",
    "        \n",
    "    for patient in data:\n",
    "\n",
    "        pid = patient['id']\n",
    "        if pid in ['P00000015', 'P00000249', 'P00000429']: # remove bad data\n",
    "            continue\n",
    "\n",
    "        fold_id = fold_dict[pid] - 1\n",
    "\n",
    "        patient_examples = []\n",
    "        patient_labels = []\n",
    "\n",
    "        if patient[zone_map].shape[-1] != patient[modality].shape[-1]: # check if segmentation map has same num slices as mri\n",
    "            continue\n",
    "\n",
    "        for slice_index in range(patient[modality].shape[-1]):\n",
    "            \n",
    "                # Check if the slice contains the prostate\n",
    "            if np.max(patient['mask'][:,:,slice_index]) > 0:\n",
    "                z_map = patient[zone_map][:,:,slice_index] > 0\n",
    "                trimmed_zone_map,indexes = trim_zeros_2D(z_map)\n",
    "                mod_slice = (patient[modality][:,:,slice_index] * z_map)[indexes[0]:indexes[1]+1,indexes[2]:indexes[3]+1]\n",
    "\n",
    "                # Perform slice-level feature extraction\n",
    "                features = feat_extraction_1d(mod_slice)\n",
    "                features.append(np.array(mod_slice))\n",
    "                \n",
    "                \n",
    "                \n",
    "                for zone_index in range(10):\n",
    "                    zone_number = zone_index + 1\n",
    "                    if zone_number in patient[zone_map][:,:,slice_index]: # check zone map to see if the slice contains the zone\n",
    "                        binary_mask = (patient[zone_map][:,:,slice_index] * z_map)[indexes[0]:indexes[1]+1,indexes[2]:indexes[3]+1] == zone_number  # create a binary mask\n",
    "\n",
    "                        # Apply masks to slices (mod and features) \n",
    "                        feat_zone = features * binary_mask # apply the mask to the slice\n",
    "                        trimmed_mod_zone, idx = trim_zeros_2D(mod_slice)\n",
    "                        for f in range(len(feat_zone)):\n",
    "                            trimmed_f_zone = feat_zone[f][idx[0]:idx[1]+1,idx[2]:idx[3]+1]# trim the slice to the dimensions of the zone num\n",
    "                            #patient_examples[f].extend(trimmed_f_zone)\n",
    "                            for item in trimmed_f_zone:\n",
    "                                examples[modality][fold_id][f].extend(np.array(item))\n",
    "                            label = [(1 if patient[label_map][slice_index][zone_index] >0 else 0)]*len(trimmed_f_zone[0])*len(trimmed_mod_zone)\n",
    "                        \n",
    "                        examples[modality][fold_id][len(feat_zone)].extend([zone_number] * len(trimmed_mod_zone) * len(trimmed_mod_zone[0])) # Add zone as a feature\n",
    "                        \n",
    "                        patient_labels.extend(label)\n",
    "                        \n",
    "                        #patient_labels.append(1 if patient[label_map][slice_index][zone_index] >0 else 0)\n",
    "                        \n",
    "        #examples[modality][fold_id].extend(patient_examples)\n",
    "        labels[fold_id].extend(patient_labels)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1781655\n"
     ]
    }
   ],
   "source": [
    "print(len(examples['ADC'][1]))\n",
    "print(len(labels[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to run the model, training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_report(model):\n",
    "    modality = 'ADC'\n",
    "    aucs = []\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "    for i in range(5): # CV Loop\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "\n",
    "        x_test = []\n",
    "        y_test = labels[i]\n",
    "        \n",
    "        for _ in range(len(examples[modality][i])):\n",
    "            x_train.append([])\n",
    "            x_test.append([])\n",
    "        \n",
    "        # Training on these\n",
    "        for j in range(5):\n",
    "            if i != j:\n",
    "                for f in range(len(examples[modality][j])):\n",
    "                    x_train[f].extend(examples[modality][j][f])\n",
    "                y_train.extend(labels[j])\n",
    "            else:\n",
    "                for f in range(len(examples[modality][i])):\n",
    "                    x_test[f].extend(examples[modality][i][f])\n",
    "        \n",
    "        x_train = np.array(x_train).T\n",
    "        x_test = np.array(x_test).T\n",
    "        \n",
    "        # TODO: feature extraction / features selection / classification here\n",
    "        \n",
    "        # Undersampling, balancing\n",
    "        x_rus, y_rus = rus.fit_sample(x_train, y_train)\n",
    "        \n",
    "        # Standardize\n",
    "        print('Standardizing...')\n",
    "        sc = StandardScaler().fit(x_rus)\n",
    "\n",
    "        stand_X = sc.transform(x_rus)\n",
    "        stand_X_test = sc.transform(x_test)\n",
    "        \n",
    "        # Train Model\n",
    "        print('Training on model... ' + model + ' - Run ' + str(i+1))\n",
    "        \n",
    "        x_df = pd.DataFrame(stand_X)\n",
    "        \n",
    "        mod = models[model]\n",
    "        mod.fit(x_df, y_rus)\n",
    "                \n",
    "        print('Evaluating on model... ' + model + ' - Run ' + str(i+1))\n",
    "                \n",
    "        y_pred = mod.predict(stand_X_test)\n",
    "                \n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        aucs.append(auc)\n",
    "        print('AUC: ' + auc)\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        # auc = auc_calculation(y_pred, y_test)\n",
    "\n",
    "    mean_auc = sum(aucs) / float(len(aucs))\n",
    "    print(mean_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing...\n",
      "Training on model... decision_tree - Run 1\n",
      "Evaluating on model... decision_tree - Run 1\n",
      "0.7277232542721817\n",
      "[[1114739  810097]\n",
      " [   2133   15112]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.58      0.73   1924836\n",
      "          1       0.02      0.88      0.04     17245\n",
      "\n",
      "avg / total       0.99      0.58      0.73   1942081\n",
      "\n",
      "Standardizing...\n",
      "Training on model... decision_tree - Run 2\n",
      "Evaluating on model... decision_tree - Run 2\n",
      "0.718652516919297\n",
      "[[1037638  728718]\n",
      " [   2297   13002]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.59      0.74   1766356\n",
      "          1       0.02      0.85      0.03     15299\n",
      "\n",
      "avg / total       0.99      0.59      0.73   1781655\n",
      "\n",
      "Standardizing...\n",
      "Training on model... decision_tree - Run 3\n",
      "Evaluating on model... decision_tree - Run 3\n",
      "0.6552531449746817\n",
      "[[922504 623339]\n",
      " [  4437  11063]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.60      0.75   1545843\n",
      "          1       0.02      0.71      0.03     15500\n",
      "\n",
      "avg / total       0.99      0.60      0.74   1561343\n",
      "\n",
      "Standardizing...\n",
      "Training on model... decision_tree - Run 4\n",
      "Evaluating on model... decision_tree - Run 4\n",
      "0.6867075510650835\n",
      "[[979667 676140]\n",
      " [  3398  12172]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.59      0.74   1655807\n",
      "          1       0.02      0.78      0.03     15570\n",
      "\n",
      "avg / total       0.99      0.59      0.74   1671377\n",
      "\n",
      "Standardizing...\n",
      "Training on model... decision_tree - Run 5\n",
      "Evaluating on model... decision_tree - Run 5\n",
      "0.7386654932579632\n",
      "[[805398 515734]\n",
      " [  2092  13721]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.61      0.76   1321132\n",
      "          1       0.03      0.87      0.05     15813\n",
      "\n",
      "avg / total       0.99      0.61      0.75   1336945\n",
      "\n",
      "0.7054003920978414\n"
     ]
    }
   ],
   "source": [
    "predict_and_report('decision_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
